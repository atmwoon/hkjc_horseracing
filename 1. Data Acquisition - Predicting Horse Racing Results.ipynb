{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Horse Racing Results\n",
    "BA Report (August 2020) <br>\n",
    "Andrew Woon\n",
    "## 1. Data Acquisition \n",
    "\n",
    "#### Required libraries\n",
    "> scrapy <br>\n",
    "> scrapy_splash <br>\n",
    "> scrapy_fake_useragent <br>\n",
    "> tabula <br>\n",
    "\n",
    "#### Required Software\n",
    "> Docker\n",
    "\n",
    "### 1.1 Data Scraping\n",
    "\n",
    "Scrapy was used as the primary data scraping library. However, as the Hong Kong Jockey Club horse racing results page (https://racing.hkjc.com/racing/information/English/racing/LocalResults.aspx) is loaded via JavaScript, the core scrapy library is insufficient and the Scrapy Splash extension was therefore needed. \n",
    "\n",
    "The Scrapy Splash library requires Docker to be installed as it uses a docker container to load the JavaScript websites and return a HTML output that scrapy is able to operate on. \n",
    "Scrapy Splash user guide reference: https://github.com/scrapy-plugins/scrapy-splash\n",
    "\n",
    "<b>File Directory </b>\n",
    "```\n",
    "└── hkjc\n",
    "    ├── hkjc\n",
    "    │   ├── __pycache__\n",
    "    │   ├── spiders\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── hkjc_spider.py\n",
    "    │   ├── items.py\n",
    "    │   ├── middlewares.py\n",
    "    │   ├── overseas_pdfparser.py\n",
    "    │   ├── overseas_rerun.py\n",
    "    │   ├── pipelines.py\n",
    "    │   └── settings.py\n",
    "    ├── hkjc_racing_fixtures.csv            #input file\n",
    "    ├── dividend_YYYYMMDDhhmm.csv           #output file format, example dividend_202008182011.csv\n",
    "    ├── horseform_YYYYMMDDhhmm.csv          #output file format\n",
    "    ├── horseinfo_YYYYMMDDhhmm.csv          #output file format\n",
    "    ├── race_YYYYMMDDhhmm.csv               #output file format\n",
    "    └── sectional_YYYYMMDDhhmm.csv          #output file format\n",
    "```\n",
    "\n",
    "Within the `./hkjc/hkjc` folder all the python files except for `overseas_pdf_parser.py` and `overseas_rerun.py` are the default scrapy required files (customised for this use case)\n",
    "\n",
    "Scrapy uses 'spiders' to crawl unique webpages and this core logic is contained in the `spider.py` file. For this use case there are 3 unique pages to crawl:\n",
    "\n",
    "| # \t| Spider \t| Webpage Description \t| Output \t|\n",
    "|:--\t|:--\t|:--\t|:--\t|\n",
    "| 1 \t| hkjc-race \t| Racing results and dividend \t| race_YYYYMMDDhhmm.csv<br>dividend_YYYYMMDDhhmm.csv \t|\n",
    "| 2 \t| hkjc-sectional \t| Sectional times from racing results \t| sectional_YYYYMMDDhhmm.csv \t|\n",
    "| 3 \t| hkjc-horse \t| Horse information \t| horseform_YYYYMMDDhhmm.csv<br>horseinfo_YYYYMMDDhhmm.csv \t|\n",
    "\n",
    "The spiders are run using the following command (in command line or powershell terminals):\n",
    "```\n",
    "scrapy crawl hkjc-race --logfile raceform_rerun.text\n",
    "```\n",
    "where `hkjc-race` can be subsituted for any of the above spiders. \n",
    "\n",
    "<b>Note:</b> scrapy-splash docker container has to be running and assumes default setting with `SPLASH_URL = 'http://localhost:8050'`, check access via browser to validate\n",
    "\n",
    "The first data scraping pass was run on the racing results page (`hkjc-race`) using manually collected dates of the past 4 racing seasons i.e. 16/17 to 19/20 stored in `hkjc_racing_fixtures.csv`. This method was used to improve scraping efficiency instead of crawling through all dates for the past 4 years. Following this the `hkjc-sectional` spider was run on the same fixture set. \n",
    "\n",
    "The scraped racing results stored in the output file `race_YYYYMMDDhhmm.csv` contains a URL to horse information, and was used by the horse information crawler `hkjc-horse`. \n",
    "\n",
    "A second data scraping pass of the horse racing results and sectional times was run on the difference between dates of the results from the first pass and `horseform_YYYYMMDDhhmm.csv` output. \n",
    "\n",
    "### 1.2 PDF Parsing\n",
    "On the horse information page active horses (not retired) contained a URL to prior overseas races which were stored as a PDF file. \n",
    "Tabula library was used to parse the horse racing results stored in the tables of the PDF into a csv file. The `overseas_pdfparser.py` takes the oversea URLs and generates the `hkjc-overseas.csv` output file. \n",
    "```\n",
    "python3 overseas_pdfparser.py\n",
    "```\n",
    "However, as the PDF parsing is unstable with some columns moved out of position or values being associated to wrong columns due to spacing issues, the following code groups outputs by number of columns and generates unformatted CSV files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scrapy\n",
    "import tabula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `overseas_rerun.csv` file contains the overseas URLs which were not successfully parsed by `overseas_pdfparser.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rerun = pd.read_csv(\"./hkjc/overseas_rerun.csv\", header = None)\n",
    "pdf_urls = list(df_rerun[df_rerun.columns[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = {}\n",
    "\n",
    "for i, pdf in enumerate(pdf_urls):\n",
    "    pdf_url = 'https://racing.hkjc.com' + pdf\n",
    "    df_pdf = tabula.read_pdf(pdf_url, stream=True)\n",
    "    df_pdf = df_pdf.iloc[1:df_pdf.index[df_pdf[df_pdf.columns[0]]==\"RACE RECORD SUMMARY\"][0],:]\n",
    "    df_pdf['overseas_url'] = pdf\n",
    "    \n",
    "    if len(df_pdf.columns) in df_d:\n",
    "        df_d[len(df_pdf.columns)] = pd.concat([df_d[len(df_pdf.columns)], df_pdf])\n",
    "    else:\n",
    "        df_d[len(df_pdf.columns)] = df_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four csv output files were generated with number of columns ranging from 17 to 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in df_d.items():\n",
    "    v.to_csv('./overseas_rerun_'+str(k)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four csv files were manually formatted so that values are associated to the correct columns. The following two functions parse this remaining data into the same format as the output from `overseas_pdfparser.py`\n",
    "\n",
    "`rerun_mergecol` function merges columns of the same attribute with similar names which were split by Tabula due to spacing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun_mergecol(df):\n",
    "    cols = ['date_venue','course_type', 'distance', 'going', 'race_name', 'group', 'position', 'draw', \n",
    "            'weight_carried', 'odds', 'gear', 'margin','jockey', 'race_time', 'race_value', \n",
    "            'prize', 'overseas_url']\n",
    "\n",
    "    for col in cols:\n",
    "        if len(df.filter(regex=col).columns) > 1:\n",
    "            df[col] = df.filter(regex=col).apply(lambda x: ''.join(x.dropna().astype(str)).strip(), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rerun_parse` function applies the same logic from `overseas_pdfparser.py` so that the same output data can be merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun_parse(df):\n",
    "    df_parse = df\n",
    "    if len(df_parse.filter(regex='Unnamed').columns)>1:\n",
    "        df_parse.columns = ['date_venue','course_type', 'distance', 'going', 'race_name', 'group', 'position', 'draw', \n",
    "                            'weight_carried', 'odds', 'gear', 'margin','jockey', 'race_time', 'race_value', \n",
    "                            'prize', 'overseas_url']\n",
    "    \n",
    "    agg_d = {'date_venue':lambda x: '|'.join(x.dropna()), 'course_type':'first', 'distance':'first', 'going':'first', \n",
    "             'race_name':lambda x: '|'.join(x.dropna()), 'group':'first', 'position':'first', 'draw':'first', \n",
    "             'weight_carried':'first', 'odds':'first', 'gear':'first', 'margin':'first', 'jockey':'first', \n",
    "             'race_time':lambda x: '|'.join(x.dropna()),'race_value':'first', 'prize':lambda x: '|'.join(x.dropna()),\n",
    "             'overseas_url': 'first'}\n",
    "    \n",
    "    df_parse = df_parse.groupby(df_parse.position.notnull().cumsum().rename(None)).agg(agg_d)\n",
    "    df_parse['date'] = df_parse['date_venue'].apply(lambda x: x[re.search(r'\\|',x).span()[1]:].strip())\n",
    "    df_parse['venue'] = df_parse['date_venue'].apply(lambda x: x[4:re.search(r'\\|',x).span()[0]].strip())\n",
    "    df_parse['prize_hkd'] = df_parse['prize'].apply(lambda x: int(x[re.search(r'\\(HKD.*?\\)',x).span()[0]+5: \\\n",
    "                                                            re.search(r'\\((HKD.*?)\\)', x).span()[1]-1].replace(',','')) \\\n",
    "                                                            if re.search(r'\\(HKD.*?\\)',x) else 0)\n",
    "    df_parse['prize'] = df_parse['prize'].apply(lambda x: x[0:re.search(r'\\d{1}\\D{2}',x).span()[0]].strip())\n",
    "    df_parse['finish_time'] = df_parse['race_time'].apply(lambda x: x[re.search(r'\\(.*?\\)',x).span()[0]+1: \\\n",
    "                                                                      re.search(r'\\(.*?\\)',x).span()[1]-1])\n",
    "    df_parse['race_time'] = df_parse['race_time'].apply(lambda x: x[0:re.search(r'\\|',x).span()[0]].strip())\n",
    "    df_parse['course_type'] = df_parse[['venue', 'course_type']].apply(lambda row: row[0][-2:] if str(row[1]) == 'nan' \\\n",
    "                                                                                               else row[1], axis=1)\n",
    "    \n",
    "    return df_parse[['date', 'venue', 'course_type', 'distance' , 'going', 'race_name', 'group', 'position', 'draw', \n",
    "                     'weight_carried', 'odds', 'gear','margin','jockey', 'race_time', 'finish_time', 'race_value', \n",
    "                     'prize', 'prize_hkd', 'overseas_url']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 manually formatted overseas rerun csvs were labelled with the suffix '\\_{no_columns}f' and passed through the above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17f\n",
      "18f\n",
      "19f\n",
      "20f\n"
     ]
    }
   ],
   "source": [
    "df_parse_d = {}\n",
    "rerun_csv = ['17f', '18f', '19f', '20f']\n",
    "for csv in rerun_csv:\n",
    "    print(csv)\n",
    "    df_rerun = pd.read_csv('overseas_rerun_'+csv+'.csv')\n",
    "    df_rerun = rerun_mergecol(df_rerun)\n",
    "    df_parse_d[csv] = rerun_parse(df_rerun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are merged together and finally merged with the prior output from `overseas_parsepdf.py` to form a merged output of all overseas race results `hkjc-overseas-merged.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for k, v in df_parse_d.items():\n",
    "    if i == 0:\n",
    "        df_rerun_merged = v\n",
    "    else:\n",
    "        df_rerun_merged = pd.concat([df_rerun_merged, v])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overseas_orig = pd.read_csv('./hkjc/hkjc-overseas.csv')\n",
    "df_overseas_merged = pd.concat([df_overseas_orig, df_rerun_merged])\n",
    "\n",
    "df_overseas_merged.to_csv('hkjc-overseas-merged.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
